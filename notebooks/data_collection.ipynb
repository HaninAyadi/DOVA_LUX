{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd70e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pprint import pprint\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "!pip install langdetect\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "from datetime import datetime, date, timedelta\n",
    "import string\n",
    "from scipy import stats\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "MONGO_HOST= '' #Fill MongoDB connexion string\n",
    "\n",
    "try:\n",
    "    client = MongoClient(MONGO_HOST)\n",
    "    db = client.database # Use database (If it doesn't exist, it will be created)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1825b9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date2timestamp(date):\n",
    "    '''\n",
    "    \"01/12/2011\"\n",
    "    '''\n",
    "    return int(time.mktime(datetime.strptime(date, \"%Y/%m/%d\").timetuple()))\n",
    "\n",
    "def next_day(date):\n",
    "  tmrw = datetime.strptime(date, \"%Y/%m/%d\") + timedelta(days=1)\n",
    "  return tmrw.strftime('%Y/%m/%d')\n",
    "\n",
    "def list_of_days(start_date, end_date):\n",
    "\n",
    "    delta = end_date - start_date       # as timedelta\n",
    "    days = []\n",
    "    for i in range(delta.days + 1):\n",
    "        day = start_date + timedelta(days=i)\n",
    "        days.append(str(day).replace('-', '/'))\n",
    "    return days\n",
    "\n",
    "def scrape_reddit(subreddit, date_start, date_end, size = 1000):\n",
    "    \"\"\" Collects posts from a given subreddit\n",
    "\n",
    "    Args:\n",
    "        subreddit: name of the subreddit from which the posts will be collected\n",
    "        date_start: start date of the collection\n",
    "        date_end: end date of the collection\n",
    "        size = {1,1000} number of posts\n",
    "\n",
    "    Returns:\n",
    "        Dataframe of the collected posts\n",
    "    \"\"\"\n",
    "    start = date2timestamp(date_start)\n",
    "    end = date2timestamp(date_end)\n",
    "    # use the pushshift api to extract out data\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?subreddit={}&sort=desc&sort_type=created_utc&after={}&before={}&size={}'.format(subreddit,start, end, size)\n",
    "    print(url)\n",
    "    try:\n",
    "        posts = requests.get(url)\n",
    "        posts = posts.json()\n",
    "        posts = posts['data']\n",
    "    except:\n",
    "        time.sleep(30)\n",
    "        posts = requests.get(url)\n",
    "        posts = posts.json()\n",
    "        posts = posts['data']\n",
    "\n",
    "    df = pd.DataFrame(columns=['subreddit', 'author', 'date', 'post',\"title\", \"upvote_ratio\"])\n",
    "    \n",
    "    for post in posts:\n",
    "        if 'selftext' in post: # check if selftext parameter exists\n",
    "            text=post['selftext']\n",
    "            if text != \"\" and  text != '[removed]' and '[deleted]' not in text: # further check if selftext is not empty\n",
    "                try: \n",
    "                    df = df.append(post, ignore_index=True)\n",
    "                except:\n",
    "                    continue\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24668b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of subreddits posts will be collected from\n",
    "subreddits = [\"covidlonghaulers\",\"Longhaulers\",\"LongCovid\",\"LongHaulersRecovery\",\n",
    "             \"longcovidsolutions\", \"LongCovidActivism\", \"Long_Covid\",\"LongCovidFighters\",\n",
    "             \"LongHaulerKids\", \"covidlonga\"]\n",
    "\n",
    "# Time period and size for scraping the subreddits\n",
    "days = list_of_days(date(2019,7,20), date(2022,8,17))\n",
    "size = 1000\n",
    "\n",
    "# Final dataframe containing all of the collected posts from the different subreddits\n",
    "subreddit_df = pd.DataFrame()\n",
    "\n",
    "# Looping through the predefined list of subreddits to scrape and save their posts\n",
    "for subreddit in subreddits:\n",
    "    days_local = list(days)\n",
    "    while subreddit_df.shape[0] < 50000 and days_local:\n",
    "        idx = random.randint(0, len(days_local)-1)\n",
    "        date_start = days_local.pop(idx)\n",
    "        date_end = next_day(date_start)\n",
    "        df = scrape_reddit(subreddit, date_start, date_end, size = size)\n",
    "        subreddit_df = pd.concat([subreddit_df, df])\n",
    "        time.sleep(0.5)\n",
    "        print(subreddit)\n",
    "        print(subreddit_df.shape)\n",
    "    # saving to the database\n",
    "    db.Reddit_Long_Covid.insert_many(subreddit_df.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1327a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e58bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
